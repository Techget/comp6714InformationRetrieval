{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP6714 Word Embeddings Demonstration using Tensorflow\n",
    "\n",
    "In this notebook, we demonstrate a basic implementation of Word Embbeddings by training the **skip-gram** model using a small test corpus: **Text8** (based on Tensorflow's word2vec tutorial). It will provide you with hands on experience prior to COMP6714 Project2. \n",
    "\n",
    "The key part of word embeddings is the training data. In this implementation, mini-batches are generated on demand, and are used by the model to update the word vectors.\n",
    "\n",
    "You are encouraged to play with this implementation, change model parameters and the training data, and analyze effect of these parameters.\n",
    "\n",
    "Note that you do not need to dig too deep into the tensorflow and the training part, though if you are into deep learning, it is good to do so.  \n",
    "\n",
    "We evaluate the quality of the embeddings by computing top-10 most nearest words for a sample of words. You will observe that semantically coherent words are embedded close to each other in the embedding space.<br>\n",
    "\n",
    "Some instructions for this notebook are given as under:<br>\n",
    "\n",
    "1. We are using **`Tensorflow 1.2.1`**, for this implementation, but this code will work for any version greater than **`1.0`**.\n",
    "2. For Tensorflow installation, please follow the link: **https://www.tensorflow.org/install/**\n",
    "3. This notebook will automatically download the training dataset: **'text8.zip'** for Word Embeddings.\n",
    "\n",
    "Detailed description of each part is given in corresponding sub-sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In next cell, we load all the requisite libraries, and modules we'll be using in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Downloading the the dataset, and reading it as a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "text8.zip\n",
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present in the working directory, and ensure that the file size is correct.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = download('text8.zip', 31344016)\n",
    "print(filename)\n",
    "\n",
    "# Function to read the data into a list of words.\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "data = read_data(filename)\n",
    "print('Data size', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build the dictionary and replace rare words with UNK token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next celll reads in the data into four global variables:\n",
    " \n",
    "1. **data** - This variable corresponds to the original text*, i.e.,* the vocabulary of the input corpus. To facilitate processing, we replace the words by corresponding integer ids (*i.e.,* from 0 to vocabulary_size-1).\n",
    "2. **count** - It is a map *(word_id x count)* used to store the occurences of words in the corpus in order to capture most frequently occuring words.\n",
    "3. **dictionary** - It is a map *(word x id)* used to store `words` and their integer `ids`.\n",
    "4. **reverse_dictionary** - It is a map *(id x word)* used to store the inverse mapping of the **dictionary**, to speed the `word/id` look-up process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5236, 3083, 12, 6, 195, 2, 3134, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 50000 # This variable is used to define the maximum vocabulary size.\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset. \n",
    "       words: a list of words, i.e., the input data\n",
    "       n_words: Vocab_size to limit the size of the vocabulary. Other words will be mapped to 'UNK'\n",
    "    \"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:  # i.e., one of the 'UNK' words\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(data, vocabulary_size)\n",
    "\n",
    "# del vocabulary  # No longer used, helps in saving space...\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next sections, we implement **skip-gram model** for word embeddings. Details about skip-gram model can be found in following paper: `https://arxiv.org/abs/1411.2738`. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generating training batch for the skip-gram model.\n",
    "\n",
    "This function is used to generate training batch for the **skip-gram model**. We are using minibatch stochastic-gradient descent to optimize the objective function. For this, we generate a training batch based on following input parameters: **batch_size**, **num_samples**, and **skip_window**.\n",
    "\n",
    "1. **batch_size** is used to control the size of the `minibatch`* of training data. i.e.,* it is the number of pairs `(CENTER_WORD, CONTEXT_WORD)`. Note that all `CENTER_WORD`s are stored and returned in the list `batch`, and all `CONTEXT_WORD`s are stored and returned in the list `label`. \n",
    "\n",
    "2. In the `skip-gram` model, we generate training data by capturing the context words from the surrondings of a target word. Here the parameter: **skip_window** defines a sliding window size of size (2*`skip_window`+1), i.e., up to `skip_window` words to the left and right of the center word.\n",
    "\n",
    "3. Within a sliding window, we need to sample **num_samples** of context words (i.e., excluding the center word). `num_samples` should be no larger than 2*`skip_window`. \n",
    "\n",
    "**Example:**<br>\n",
    "As an example, with **batch_size**= 8, **skip_window** = 1, and **num_samples** = 2, it generates following data batch. We consider these words as `(input,output)` pairs for the `skip-gram` model.\n",
    "\n",
    "**Text:** `anarchism originated as a term of abuse first used against early` <br>\n",
    "\n",
    "**`(CENTER_WORD, CONTEXT_WORD)` Pairs:**<br>\n",
    " `(originated,anarchism)`, `(originated,as)`, `(as,originated)`, `(as,a)`, `(a,term)`\n",
    " `(a,as)`, `(term,of)`, `(term,a)`\n",
    " \n",
    "**Explanations:**\n",
    "\n",
    "1. We choose to always generate `num_skip` pairs for each center word. Therefore, \n",
    "    * We only need to consider (8/2) center words. \n",
    "    * We start from the second word (so that it has a full-sized sliding window). \n",
    "2. With **skip_window** = 1, **num_samples** = 2, we  generate two samples from two context words (in random order).\n",
    "3. The implementation here is a slightly modified version from tensorflow's word2vec tutorial code; it is still a very ugly implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0:10] = ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "\n",
      ".. First batch\n",
      "data_index = 0, buffer = ['anarchism', 'originated', 'as']\n",
      "data_index = 4, buffer = ['originated', 'as', 'a']\n",
      "data_index = 5, buffer = ['as', 'a', 'term']\n",
      "data_index = 6, buffer = ['a', 'term', 'of']\n",
      "data_index = 7, buffer = ['term', 'of', 'abuse']\n",
      "originated -> as\n",
      "originated -> anarchism\n",
      "as -> originated\n",
      "as -> a\n",
      "a -> term\n",
      "a -> as\n",
      "term -> of\n",
      "term -> a\n",
      "4\n",
      "\n",
      ".. Second batch\n",
      "data_index = 4, buffer = ['term', 'of', 'abuse']\n",
      "data_index = 8, buffer = ['of', 'abuse', 'first']\n",
      "data_index = 9, buffer = ['abuse', 'first', 'used']\n",
      "data_index = 10, buffer = ['first', 'used', 'against']\n",
      "data_index = 11, buffer = ['used', 'against', 'early']\n",
      "of -> term\n",
      "of -> abuse\n",
      "abuse -> of\n",
      "abuse -> first\n",
      "first -> abuse\n",
      "first -> used\n",
      "used -> against\n",
      "used -> first\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "data_index = 0 \n",
    "# the variable is abused in this implementation. \n",
    "# Outside the sample generation loop, it is the position of the sliding window: from data_index to data_index + span\n",
    "# Inside the sample generation loop, it is the next word to be added to a size-limited buffer. \n",
    "\n",
    "def generate_batch(batch_size, num_samples, skip_window):\n",
    "    global data_index   \n",
    "    \n",
    "    assert batch_size % num_samples == 0\n",
    "    assert num_samples <= 2 * skip_window\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # span is the width of the sliding window\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span]) # initial buffer content = first sliding window\n",
    "    \n",
    "    print('data_index = {}, buffer = {}'.format(data_index, [reverse_dictionary[w] for w in buffer]))\n",
    "\n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_samples):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        random.shuffle(context_words)\n",
    "        words_to_use = collections.deque(context_words) # now we obtain a random list of context words\n",
    "        for j in range(num_samples): # generate the training pairs\n",
    "            batch[i * num_samples + j] = buffer[skip_window]\n",
    "            context_word = words_to_use.pop()\n",
    "            labels[i * num_samples + j, 0] = buffer[context_word] # buffer[context_word] is a random context word\n",
    "        \n",
    "        # slide the window to the next position    \n",
    "        if data_index == len(data):\n",
    "            buffer = data[:span]\n",
    "            data_index = span\n",
    "        else: \n",
    "            buffer.append(data[data_index]) # note that due to the size limit, the left most word is automatically removed from the buffer.\n",
    "            data_index += 1\n",
    "        \n",
    "        print('data_index = {}, buffer = {}'.format(data_index, [reverse_dictionary[w] for w in buffer]))\n",
    "        \n",
    "    # end-of-for\n",
    "    data_index = (data_index + len(data) - span) % len(data) # move data_index back by `span`\n",
    "    return batch, labels\n",
    "\n",
    "# \n",
    "print('data[0:10] = {}'.format([reverse_dictionary[i] for i in data[:10]]))\n",
    "\n",
    "print('\\n.. First batch')\n",
    "batch, labels = generate_batch(batch_size=8, num_samples=2, skip_window=1)\n",
    "for i in range(8):\n",
    "    print(reverse_dictionary[batch[i]], '->', reverse_dictionary[labels[i, 0]])\n",
    "print(data_index)\n",
    "    \n",
    "print('\\n.. Second batch')\n",
    "batch, labels = generate_batch(batch_size=8, num_samples=2, skip_window=1)\n",
    "for i in range(8):\n",
    "    print(reverse_dictionary[batch[i]], '->', reverse_dictionary[labels[i, 0]])\n",
    "print(data_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specification of Training data:\n",
    "batch_size = 128      # Size of mini-batch for skip-gram model.\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right of the target word.\n",
    "num_samples = 2         # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64      # Sample size for negative examples.\n",
    "logs_path = './log/'\n",
    "\n",
    "# Specification of test Sample:\n",
    "sample_size = 20       # Random sample of words to evaluate similarity.\n",
    "sample_window = 100    # Only pick samples in the head of the distribution.\n",
    "sample_examples = np.random.choice(sample_window, sample_size, replace=False) # Randomly pick a sample of size 16\n",
    "\n",
    "## Constructing the graph...\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        # Placeholders to read input data.\n",
    "        with tf.name_scope('Inputs'):\n",
    "            train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "            train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "            \n",
    "        # Look up embeddings for inputs.\n",
    "        with tf.name_scope('Embeddings'):            \n",
    "            sample_dataset = tf.constant(sample_examples, dtype=tf.int32)\n",
    "            embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "            \n",
    "            # Construct the variables for the NCE loss\n",
    "            nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                                      stddev=1.0 / math.sqrt(embedding_size)))\n",
    "            nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        \n",
    "        # Compute the average NCE loss for the batch.\n",
    "        # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "        # time we evaluate the loss.\n",
    "        with tf.name_scope('Loss'):\n",
    "            loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights, biases=nce_biases, \n",
    "                                             labels=train_labels, inputs=embed, \n",
    "                                             num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "        \n",
    "        # Construct the Gradient Descent optimizer using a learning rate of 0.01.\n",
    "        with tf.name_scope('Gradient_Descent'):\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1).minimize(loss)\n",
    "\n",
    "        # Normalize the embeddings to avoid overfitting.\n",
    "        with tf.name_scope('Normalization'):\n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "            normalized_embeddings = embeddings / norm\n",
    "            \n",
    "        sample_embeddings = tf.nn.embedding_lookup(normalized_embeddings, sample_dataset)\n",
    "        similarity = tf.matmul(sample_embeddings, normalized_embeddings, transpose_b=True)\n",
    "        \n",
    "        # Add variable initializer.\n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        \n",
    "        # Create a summary to monitor cost tensor\n",
    "        tf.summary.scalar(\"cost\", loss)\n",
    "        # Merge all summary variables.\n",
    "        merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Begin training\n",
    "\n",
    "In order to execute the model, we initialize a session object using `tf.Session()`, and call respective node via `session.run()` or `eval()`. General workflow for the training process is as under:\n",
    "\n",
    "1. Define number of training steps.\n",
    "2. Initialize all variables*, i.e,* embeddings, weights and biases using `session.run(init)`.\n",
    "3. Placeholder, (`train_inputs`, and `train_labels`) are used to feed input data to skip-gram using the method `generate_batch`\n",
    "4. `optimizer`, and `loss` are executed by calling `session.run()`\n",
    "5. Print out average loss after every **5000** iterations\n",
    "6. Evaluate the **similarity** after every **10000** iterations. Look for top-10 nearest neighbours for words in sample set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the model\n",
      "Nearest to american: instrument, andechs, akc, kahanamoku, dup, sahara, sandwich, irons, lupus, buffy,\n",
      "Nearest to that: angelica, wreckin, cannibalism, samba, innodb, clinically, interrogators, fueled, igf, goldsmith,\n",
      "Nearest to may: journal, asf, insubstantial, attached, cve, platz, rendering, managed, ign, fibonacci,\n",
      "Nearest to by: dhul, syncopation, manifestation, chekhov, backstreet, colonist, appalachian, pseudopods, dioscorus, bother,\n",
      "Nearest to into: clusters, sarai, platonic, flintlock, nationalisation, ordering, addressable, handgun, ketchup, niro,\n",
      "Nearest to first: cleavage, swell, polydor, tiamat, consciously, reentry, bovary, piercing, volumes, ruthenia,\n",
      "Nearest to while: plucked, pomerania, corundum, gels, ambiguous, supersonic, scrooge, spain, weishaupt, existentialist,\n",
      "Nearest to such: modem, chill, wine, substandard, necessary, performance, investigates, umami, clermont, downwards,\n",
      "Nearest to than: hardliners, decrypt, metastability, caracas, airman, intel, foxy, clan, sermons, scientific,\n",
      "Nearest to called: brahmic, hj, bolster, abodes, vietnamese, modalities, idealist, australes, social, burnham,\n",
      "Nearest to no: xs, eigenstates, clades, marc, chivalry, traffickers, parsecs, flirty, vipers, empty,\n",
      "Nearest to some: var, aether, citeaux, expire, bes, buell, mecca, robbie, stylistically, stipulates,\n",
      "Nearest to would: heralds, prosperous, refusing, rewind, orang, franke, trypomastigotes, eager, fleas, expropriated,\n",
      "Nearest to world: limburg, predatory, apologies, maynard, rectangles, scorn, munk, yf, oiled, andaman,\n",
      "Nearest to there: ueshiba, characterise, berlin, intrigue, palatalized, nationale, spoil, attackers, punishable, entrepreneurship,\n",
      "Nearest to when: kant, schroeder, reactivated, slums, rejoice, swearing, arran, harmful, alen, franz,\n",
      "Nearest to time: eigenvectors, nuba, nubia, escaped, delusions, tapioca, molars, metallica, bangla, elmira,\n",
      "Nearest to at: rh, hamlet, rsted, nesting, premiums, arsenal, propped, stacey, capitan, gerhard,\n",
      "Nearest to not: equinoxes, charmed, aug, palm, gnome, unger, metaphorical, mistakenly, stifle, aegis,\n",
      "Nearest to one: prevented, rematch, proportions, scilly, sotho, jaffa, trespass, cattle, firewalls, chlorine,\n",
      "\n",
      "Average loss at step  5000 :  73.3854969249\n",
      "Average loss at step  10000 :  22.9296655355\n",
      "Nearest to american: instrument, agave, financial, partnership, simplified, sahara, coral, alcohols, sx, computer,\n",
      "Nearest to that: and, maguey, vs, alpina, neuron, anthropology, blitz, android, protests, in,\n",
      "Nearest to may: attached, journal, managed, lists, conversation, vs, rendering, seen, tension, note,\n",
      "Nearest to by: in, and, is, to, sod, was, manufactures, with, for, coke,\n",
      "Nearest to into: platonic, camus, clusters, ordering, bus, with, of, stations, verde, locke,\n",
      "Nearest to first: archie, of, refrigerator, volumes, vs, agave, terra, cca, event, cambodia,\n",
      "Nearest to while: service, central, dddddd, ambiguous, accessible, desert, spassky, sum, spain, ha,\n",
      "Nearest to such: the, necessary, modem, performance, a, wine, vs, monotheism, jedwabne, clock,\n",
      "Nearest to than: and, milne, sod, simplest, intel, prior, scientific, clan, older, phi,\n",
      "Nearest to called: brahmic, vietnamese, gb, coke, defensive, gradually, social, parties, month, scholar,\n",
      "Nearest to no: gland, hundreds, neolithic, empty, female, parry, marc, archie, win, harlem,\n",
      "Nearest to some: var, players, basins, controlling, archie, this, souls, agave, snakes, sees,\n",
      "Nearest to would: prosperous, ibn, text, quarterback, strategy, to, aberdeen, cases, finalist, gland,\n",
      "Nearest to world: gb, jpg, volume, trials, execution, predatory, pka, brandy, transcends, pulling,\n",
      "Nearest to there: berlin, ueshiba, gch, trend, andrew, motile, degree, fbi, vs, good,\n",
      "Nearest to when: july, harmful, region, none, franz, hiroshima, kant, arran, folded, sure,\n",
      "Nearest to time: escaped, hold, thought, apparent, nubia, chronic, nuremberg, agave, charges, models,\n",
      "Nearest to at: in, and, is, with, of, one, coke, macleod, vaughan, three,\n",
      "Nearest to not: coke, plot, excavation, mistakenly, caught, palm, frame, bin, acres, inherent,\n",
      "Nearest to one: nine, two, archie, zero, three, gb, vs, phi, six, aberdeenshire,\n",
      "\n",
      "Average loss at step  15000 :  12.4232016604\n",
      "Average loss at step  20000 :  8.57796830249\n",
      "Nearest to american: and, instrument, agave, english, simplified, sx, financial, apologia, b, newly,\n",
      "Nearest to that: which, and, then, maguey, pigweed, alpina, acacia, vs, it, blitz,\n",
      "Nearest to may: would, attached, tuva, journal, eight, conversation, lists, diffuse, nine, but,\n",
      "Nearest to by: in, was, is, and, for, with, five, from, as, to,\n",
      "Nearest to into: with, at, platonic, and, from, sarai, in, camus, bus, verde,\n",
      "Nearest to first: archie, of, in, terra, volumes, refrigerator, agave, event, cca, cambodia,\n",
      "Nearest to while: and, pomerania, when, desert, corundum, central, sociological, is, on, accessible,\n",
      "Nearest to such: alpaca, modem, necessary, performance, monotheism, circulation, seven, clock, virginity, default,\n",
      "Nearest to than: and, milne, sod, five, simplest, intel, factions, scientific, antoninus, or,\n",
      "Nearest to called: vietnamese, brahmic, coke, gb, antwerp, and, dasyprocta, av, homomorphism, defensive,\n",
      "Nearest to no: hundreds, neolithic, gland, empty, female, integrative, tuning, parry, three, marc,\n",
      "Nearest to some: the, var, this, citeaux, agouti, its, controlling, players, snakes, charts,\n",
      "Nearest to would: to, can, prosperous, may, fare, isu, quarterback, cases, text, strategy,\n",
      "Nearest to world: volume, gb, trials, insights, agouti, jpg, peptide, execution, predatory, pulling,\n",
      "Nearest to there: it, berlin, they, ueshiba, trend, gch, he, and, punishable, luther,\n",
      "Nearest to when: was, on, while, alen, harmful, is, in, eight, july, for,\n",
      "Nearest to time: nubia, escaped, chronic, hold, thought, dasyprocta, nsw, nuremberg, apparent, microscopy,\n",
      "Nearest to at: in, and, on, with, for, apologia, circ, of, agouti, vaughan,\n",
      "Nearest to not: coke, to, plot, it, also, caught, excavation, three, mistakenly, bin,\n",
      "Nearest to one: two, agouti, three, eight, nine, five, seven, dasyprocta, circ, six,\n",
      "\n",
      "Average loss at step  25000 :  6.96411557055\n",
      "Average loss at step  30000 :  6.23746108785\n",
      "Nearest to american: and, instrument, simplified, agave, english, b, barfield, amalthea, financial, overlaps,\n",
      "Nearest to that: which, bpp, then, it, azad, this, and, drones, pigweed, amalthea,\n",
      "Nearest to may: would, abet, could, can, tuva, diffuse, attached, eight, nine, but,\n",
      "Nearest to by: in, with, from, was, for, and, is, as, six, five,\n",
      "Nearest to into: from, at, with, sarai, in, by, platonic, and, minute, sponsors,\n",
      "Nearest to first: archie, agave, terra, agouti, otimes, refrigerator, in, cca, current, extrasolar,\n",
      "Nearest to while: and, pomerania, when, sociological, corundum, central, at, desert, service, weishaupt,\n",
      "Nearest to such: modem, alpaca, anomalous, well, monotheism, downwards, default, necessary, virginity, circulation,\n",
      "Nearest to than: and, or, milne, factions, sod, intel, simplest, reactive, caused, scientific,\n",
      "Nearest to called: and, vietnamese, brahmic, coke, gb, antwerp, dasyprocta, homomorphism, smuggling, gradually,\n",
      "Nearest to no: hundreds, neolithic, abet, tuning, empty, female, integrative, ever, gland, often,\n",
      "Nearest to some: abet, var, this, many, the, agouti, its, citeaux, flowered, controlling,\n",
      "Nearest to would: can, may, to, prosperous, could, isu, had, fare, should, will,\n",
      "Nearest to world: insights, volume, trials, scorn, gb, pulling, brandy, agouti, peptide, execution,\n",
      "Nearest to there: it, they, he, berlin, trend, not, ueshiba, frescoes, gch, abet,\n",
      "Nearest to when: was, while, for, five, eight, in, is, seven, six, otimes,\n",
      "Nearest to time: tapioca, chronic, nubia, escaped, hold, hotels, delusions, thought, nsw, amalthea,\n",
      "Nearest to at: in, on, with, and, for, amalthea, apologia, circ, three, agouti,\n",
      "Nearest to not: to, it, also, coke, caught, bpp, excavation, often, mistakenly, plot,\n",
      "Nearest to one: two, four, eight, agouti, seven, three, circ, six, amalthea, five,\n",
      "\n",
      "Average loss at step  35000 :  5.81930053782\n",
      "Average loss at step  40000 :  5.43187191138\n",
      "Nearest to american: and, instrument, agave, simplified, english, barfield, amalthea, sandwich, akc, british,\n",
      "Nearest to that: which, bpp, this, then, but, however, it, when, four, neuron,\n",
      "Nearest to may: would, can, could, abet, will, tuva, should, diffuse, attached, eight,\n",
      "Nearest to by: was, with, from, in, is, be, zero, as, and, were,\n",
      "Nearest to into: from, with, at, sarai, handgun, lucent, minute, under, raided, coke,\n",
      "Nearest to first: archie, agave, otimes, current, agouti, terra, invaluable, extrasolar, refrigerator, cca,\n",
      "Nearest to while: and, when, pomerania, sociological, is, on, scrooge, was, corundum, supersonic,\n",
      "Nearest to such: well, alpaca, modem, monotheism, mosiah, anomalous, downwards, approaching, consoles, circulation,\n",
      "Nearest to than: or, milne, and, factions, detained, sod, intel, reactive, dennis, five,\n",
      "Nearest to called: UNK, vietnamese, antwerp, brahmic, coke, gb, and, bolster, smuggling, dasyprocta,\n",
      "Nearest to no: hundreds, neolithic, abet, tuning, empty, integrative, female, ever, often, a,\n",
      "Nearest to some: abet, many, this, its, the, agouti, var, flowered, citeaux, these,\n",
      "Nearest to would: can, may, to, could, will, should, prosperous, isu, eight, renovate,\n",
      "Nearest to world: insights, volume, scorn, trials, brandy, pulling, gb, predatory, bytes, teamed,\n",
      "Nearest to there: it, they, he, berlin, not, trend, which, and, gch, frescoes,\n",
      "Nearest to when: while, was, that, on, is, seven, six, four, eight, five,\n",
      "Nearest to time: albury, tapioca, chronic, amalthea, thought, hold, hotels, nubia, delusions, chlorophyll,\n",
      "Nearest to at: in, on, with, amalthea, apologia, and, circ, from, zero, for,\n",
      "Nearest to not: it, also, to, coke, often, they, bpp, caught, excavation, there,\n",
      "Nearest to one: two, eight, four, three, six, zero, seven, five, agouti, circ,\n",
      "\n",
      "Average loss at step  45000 :  5.30632296114\n",
      "Average loss at step  50000 :  5.12967471261\n",
      "Nearest to american: and, agave, instrument, english, kennel, simplified, amalthea, british, barfield, french,\n",
      "Nearest to that: which, this, bpp, then, kapoor, however, but, it, alphorn, when,\n",
      "Nearest to may: would, can, could, will, might, abet, should, tuva, cannot, eight,\n",
      "Nearest to by: was, with, in, from, is, be, for, eventually, as, and,\n",
      "Nearest to into: from, with, at, nationalisation, in, sarai, handgun, under, lucent, sponsors,\n",
      "Nearest to first: archie, invaluable, next, last, agave, otimes, current, agouti, in, extrasolar,\n",
      "Nearest to while: and, when, pomerania, sociological, was, but, on, is, spassky, eight,\n",
      "Nearest to such: well, monotheism, downwards, alpaca, mosiah, modem, anomalous, consoles, known, responsa,\n",
      "Nearest to than: and, or, milne, factions, detained, sod, reactive, phi, intel, galician,\n",
      "Nearest to called: UNK, antwerp, gb, brahmic, vietnamese, and, coke, bolster, smuggling, dasyprocta,\n",
      "Nearest to no: hundreds, ever, neolithic, empty, often, tuning, a, abet, integrative, it,\n",
      "Nearest to some: many, abet, this, these, its, agouti, flowered, var, the, any,\n",
      "Nearest to would: can, may, could, will, to, should, isu, prosperous, did, eight,\n",
      "Nearest to world: insights, scorn, volume, stumbling, pulling, trials, pseudocode, brandy, fida, maynard,\n",
      "Nearest to there: they, it, he, not, which, berlin, frescoes, abet, gch, trend,\n",
      "Nearest to when: while, was, eight, for, if, four, six, seven, and, in,\n",
      "Nearest to time: tapioca, albury, hotels, delusions, amalthea, chlorophyll, hold, battalions, chronic, escaped,\n",
      "Nearest to at: in, on, with, for, amalthea, three, agouti, apologia, during, and,\n",
      "Nearest to not: it, also, they, often, you, to, coke, ruth, caught, bpp,\n",
      "Nearest to one: two, three, four, six, eight, five, seven, agouti, kapoor, circ,\n",
      "\n",
      "Average loss at step  55000 :  5.10098364229\n",
      "Average loss at step  60000 :  5.01359458108\n",
      "Nearest to american: and, british, agave, simplified, english, instrument, kennel, french, barfield, amalthea,\n",
      "Nearest to that: which, this, bpp, however, then, but, kapoor, when, also, alphorn,\n",
      "Nearest to may: would, can, could, will, might, should, abet, cannot, tuva, must,\n",
      "Nearest to by: was, be, with, as, five, eventually, been, from, agouti, against,\n",
      "Nearest to into: from, nationalisation, at, under, with, sarai, against, lucent, in, handgun,\n",
      "Nearest to first: archie, next, invaluable, last, agouti, agave, current, second, otimes, event,\n",
      "Nearest to while: when, and, pulau, pomerania, but, was, sociological, is, acacia, gpo,\n",
      "Nearest to such: well, downwards, known, mosiah, monotheism, consoles, alpaca, anomalous, modem, responsa,\n",
      "Nearest to than: or, and, microsite, milne, factions, detained, sod, cooperating, reactive, quiz,\n",
      "Nearest to called: UNK, michelob, antwerp, and, vietnamese, brahmic, gb, bolster, smuggling, coke,\n",
      "Nearest to no: a, hundreds, empty, ever, often, it, five, neolithic, appealing, abet,\n",
      "Nearest to some: many, abet, these, this, its, several, agouti, three, the, two,\n",
      "Nearest to would: can, may, could, will, to, should, did, isu, had, might,\n",
      "Nearest to world: insights, umlaut, scorn, stumbling, maynard, pulling, volume, pseudocode, trials, brandy,\n",
      "Nearest to there: it, they, he, which, not, ursus, abet, often, trend, consoles,\n",
      "Nearest to when: while, if, six, was, seven, for, eight, four, pulau, is,\n",
      "Nearest to time: tapioca, sylia, delusions, albury, hotels, battalions, amalthea, michelob, chlorophyll, theron,\n",
      "Nearest to at: in, on, michelob, during, ursus, amalthea, with, apologia, agouti, six,\n",
      "Nearest to not: it, they, to, also, often, you, coke, ruth, caught, generally,\n",
      "Nearest to one: two, six, three, five, ursus, four, agouti, seven, eight, kapoor,\n",
      "\n",
      "Average loss at step  65000 :  4.87570189486\n",
      "Average loss at step  70000 :  4.8677266675\n",
      "Nearest to american: british, simplified, agave, french, instrument, english, kennel, and, barfield, arctos,\n",
      "Nearest to that: which, this, bpp, kapoor, callithrix, however, then, microcebus, but, upanija,\n",
      "Nearest to may: would, can, could, will, might, should, must, abet, cannot, tuva,\n",
      "Nearest to by: was, be, upanija, eventually, thaler, with, as, in, elective, sod,\n",
      "Nearest to into: from, under, nationalisation, with, at, sarai, against, lucent, mico, on,\n",
      "Nearest to first: archie, next, second, last, current, agave, invaluable, agouti, otimes, thaler,\n",
      "Nearest to while: when, and, but, pulau, was, pomerania, sociological, michelob, upanija, although,\n",
      "Nearest to such: well, downwards, known, mosiah, many, these, consoles, bracing, monotheism, alpaca,\n",
      "Nearest to than: or, microsite, milne, detained, and, cooperating, factions, sod, thaler, reactive,\n",
      "Nearest to called: UNK, michelob, antwerp, smuggling, coke, gb, brahmic, ursus, vietnamese, bolster,\n",
      "Nearest to no: empty, often, hundreds, ever, upanija, logically, another, neolithic, appealing, it,\n",
      "Nearest to some: many, abet, these, callithrix, several, the, agouti, bracing, this, any,\n",
      "Nearest to would: can, may, could, will, to, should, might, did, isu, must,\n",
      "Nearest to world: insights, stumbling, umlaut, scorn, volume, maynard, pulling, pseudocode, trials, customizable,\n",
      "Nearest to there: they, it, he, still, not, often, callithrix, mico, which, usually,\n",
      "Nearest to when: while, if, was, before, for, six, upanija, as, four, seven,\n",
      "Nearest to time: tapioca, delusions, battalions, microcebus, amalthea, hotels, sylia, albury, theron, chlorophyll,\n",
      "Nearest to at: in, on, michelob, during, callithrix, apologia, three, amalthea, thaler, ursus,\n",
      "Nearest to not: it, they, often, to, you, generally, also, coke, ruth, bpp,\n",
      "Nearest to one: six, two, five, four, seven, three, ursus, eight, agouti, microcebus,\n",
      "\n",
      "Average loss at step  75000 :  4.78121066928\n",
      "Average loss at step  80000 :  4.78013677206\n",
      "Nearest to american: british, french, english, simplified, agave, instrument, kennel, and, barfield, thibetanus,\n",
      "Nearest to that: which, this, however, then, bpp, callithrix, kapoor, but, microcebus, alphorn,\n",
      "Nearest to may: can, would, could, will, might, should, must, cannot, abet, nine,\n",
      "Nearest to by: was, be, upanija, eventually, from, thaler, in, with, were, elective,\n",
      "Nearest to into: from, under, nationalisation, with, against, in, sarai, lucent, mico, sponsors,\n",
      "Nearest to first: archie, second, last, next, agave, invaluable, agouti, current, otimes, upanija,\n",
      "Nearest to while: when, and, but, pulau, crb, pomerania, however, although, sociological, michelob,\n",
      "Nearest to such: well, these, many, known, mosiah, downwards, wraith, consoles, some, alpaca,\n",
      "Nearest to than: or, and, microsite, milne, detained, cooperating, sod, factions, thaler, but,\n",
      "Nearest to called: UNK, michelob, antwerp, and, ursus, smuggling, gb, coke, divide, bolster,\n",
      "Nearest to no: ever, often, hundreds, empty, upanija, it, neolithic, another, agp, appealing,\n",
      "Nearest to some: many, these, abet, several, callithrix, manure, this, other, bracing, agouti,\n",
      "Nearest to would: can, may, will, could, to, should, might, must, did, isu,\n",
      "Nearest to world: insights, scorn, stumbling, umlaut, volume, maynard, pulling, trials, pseudocode, fida,\n",
      "Nearest to there: they, it, he, which, often, still, not, usually, callithrix, now,\n",
      "Nearest to when: while, if, before, was, six, however, upanija, as, where, seven,\n",
      "Nearest to time: tapioca, delusions, battalions, microcebus, amalthea, chlorophyll, theron, ursus, sylia, hotels,\n",
      "Nearest to at: in, on, during, michelob, amalthea, thaler, polyn, ursus, callithrix, from,\n",
      "Nearest to not: it, often, generally, they, to, you, also, there, ruth, bpp,\n",
      "Nearest to one: seven, two, six, five, three, four, ursus, agouti, eight, microcebus,\n",
      "\n",
      "Average loss at step  85000 :  4.78658102405\n",
      "Average loss at step  90000 :  4.72901453767\n",
      "Nearest to american: british, french, english, simplified, instrument, agave, kennel, barfield, thibetanus, amalthea,\n",
      "Nearest to that: which, however, but, this, bpp, callithrix, then, kapoor, it, microcebus,\n",
      "Nearest to may: can, would, could, will, might, should, must, cannot, abet, to,\n",
      "Nearest to by: was, upanija, as, with, be, thaler, eventually, from, foodborne, when,\n",
      "Nearest to into: from, under, nationalisation, with, through, against, sarai, troll, lucent, mico,\n",
      "Nearest to first: second, last, archie, next, invaluable, agave, agouti, otimes, only, current,\n",
      "Nearest to while: when, and, but, pulau, however, although, crb, michelob, pomerania, mitral,\n",
      "Nearest to such: well, these, many, known, mosiah, downwards, responsa, selamat, some, wraith,\n",
      "Nearest to than: or, microsite, milne, and, but, detained, cooperating, thaler, factions, sod,\n",
      "Nearest to called: vert, UNK, michelob, antwerp, ursus, and, gb, pulau, supercar, smuggling,\n",
      "Nearest to no: often, ever, upanija, a, hundreds, another, empty, any, semiconductors, appealing,\n",
      "Nearest to some: many, these, abet, several, the, callithrix, manure, other, bracing, this,\n",
      "Nearest to would: can, may, will, could, should, might, to, must, did, does,\n",
      "Nearest to world: insights, scorn, stumbling, umlaut, maynard, volume, pulling, trials, customizable, pseudocode,\n",
      "Nearest to there: they, it, he, often, which, not, mico, still, usually, callithrix,\n",
      "Nearest to when: if, while, before, for, as, was, where, after, however, during,\n",
      "Nearest to time: peacocks, battalions, tapioca, delusions, chlorophyll, theron, microcebus, amalthea, hotels, sylia,\n",
      "Nearest to at: in, on, during, michelob, thaler, ursus, callithrix, amalthea, polyn, from,\n",
      "Nearest to not: they, often, generally, it, you, there, bpp, also, to, finalist,\n",
      "Nearest to one: two, four, three, seven, five, eight, six, ursus, crb, agouti,\n",
      "\n",
      "Average loss at step  95000 :  4.68849103527\n",
      "Average loss at step  100000 :  4.65652717667\n",
      "Nearest to american: british, french, english, simplified, instrument, and, agave, kennel, barfield, thibetanus,\n",
      "Nearest to that: which, however, but, this, bpp, callithrix, then, kapoor, what, microcebus,\n",
      "Nearest to may: can, would, could, will, might, should, must, cannot, abet, did,\n",
      "Nearest to by: was, be, upanija, thaler, with, as, eventually, foodborne, gyeongsang, elective,\n",
      "Nearest to into: from, under, nationalisation, through, with, sponsors, lucent, mico, troll, against,\n",
      "Nearest to first: second, last, next, archie, agouti, invaluable, current, agave, otimes, upanija,\n",
      "Nearest to while: when, but, and, although, however, pulau, crb, after, michelob, mitral,\n",
      "Nearest to such: well, these, many, known, some, mosiah, selamat, downwards, having, responsa,\n",
      "Nearest to than: or, and, microsite, milne, but, detained, thaler, cooperating, factions, sod,\n",
      "Nearest to called: vert, and, michelob, antwerp, supercar, ursus, pulau, UNK, smuggling, considered,\n",
      "Nearest to no: ever, any, upanija, often, another, nine, empty, hundreds, semiconductors, agp,\n",
      "Nearest to some: many, these, abet, several, callithrix, the, this, manure, any, other,\n",
      "Nearest to would: can, may, will, could, should, might, must, to, did, does,\n",
      "Nearest to world: insights, scorn, stumbling, umlaut, pulling, volume, maynard, trials, customizable, peacocks,\n",
      "Nearest to there: they, it, he, often, now, which, mico, still, usually, callithrix,\n",
      "Nearest to when: if, while, before, after, where, during, since, as, for, however,\n",
      "Nearest to time: peacocks, battalions, amalthea, delusions, microcebus, tapioca, chlorophyll, year, hotels, theron,\n",
      "Nearest to at: in, during, michelob, polyn, thaler, ursus, with, on, callithrix, amalthea,\n",
      "Nearest to not: generally, they, often, it, to, bpp, also, you, there, ruth,\n",
      "Nearest to one: six, two, seven, four, five, three, eight, iit, agouti, ursus,\n",
      "\n",
      "Average loss at step  105000 :  4.62918047304\n",
      "Average loss at step  110000 :  4.61428171692\n",
      "Nearest to american: british, english, french, agave, instrument, kennel, simplified, and, thibetanus, barfield,\n",
      "Nearest to that: which, however, but, bpp, this, callithrix, kapoor, then, when, what,\n",
      "Nearest to may: can, would, could, will, might, should, must, cannot, abet, did,\n",
      "Nearest to by: upanija, with, be, was, during, against, thaler, seven, as, without,\n",
      "Nearest to into: from, under, through, with, nationalisation, troll, sponsors, against, to, lucent,\n",
      "Nearest to first: second, last, next, archie, agave, current, agouti, invaluable, upanija, otimes,\n",
      "Nearest to while: when, but, although, however, and, pulau, crb, mitral, after, michelob,\n",
      "Nearest to such: well, these, many, known, having, some, including, mosiah, downwards, responsa,\n",
      "Nearest to than: or, and, microsite, milne, but, cooperating, detained, thaler, much, factions,\n",
      "Nearest to called: vert, michelob, antwerp, ursus, supercar, pulau, bokassa, callithrix, UNK, earns,\n",
      "Nearest to no: any, ever, another, upanija, empty, semiconductors, agp, hundreds, appealing, often,\n",
      "Nearest to some: many, these, several, abet, callithrix, the, all, any, manure, other,\n",
      "Nearest to would: will, can, may, could, might, should, must, did, to, does,\n",
      "Nearest to world: insights, scorn, stumbling, pulling, umlaut, customizable, maynard, trials, peacocks, volume,\n",
      "Nearest to there: they, it, he, still, often, which, now, usually, mico, not,\n",
      "Nearest to when: if, while, before, after, where, however, during, but, although, as,\n",
      "Nearest to time: peacocks, battalions, delusions, amalthea, year, microcebus, tapioca, kapoor, ursus, hotels,\n",
      "Nearest to at: in, during, with, from, thaler, on, michelob, polyn, amalthea, ursus,\n",
      "Nearest to not: generally, often, they, it, bpp, to, you, also, there, ruth,\n",
      "Nearest to one: seven, two, six, five, four, three, crb, ursus, eight, agouti,\n",
      "\n",
      "Average loss at step  115000 :  4.5636058845\n",
      "Average loss at step  120000 :  4.52075491862\n",
      "Nearest to american: british, english, french, agave, kennel, simplified, adjudged, instrument, barfield, european,\n",
      "Nearest to that: which, however, bpp, this, kapoor, what, callithrix, but, netbios, alphorn,\n",
      "Nearest to may: can, would, could, will, might, should, must, cannot, abet, tuva,\n",
      "Nearest to by: upanija, was, with, be, thaler, through, against, eventually, without, from,\n",
      "Nearest to into: from, through, under, with, nationalisation, troll, against, sponsors, mico, arctos,\n",
      "Nearest to first: second, last, next, archie, current, agave, agouti, invaluable, upanija, brassicaceae,\n",
      "Nearest to while: when, and, although, but, however, pulau, crb, mitral, after, michelob,\n",
      "Nearest to such: well, these, many, known, some, including, having, responsa, avalanches, mosiah,\n",
      "Nearest to than: or, but, and, milne, microsite, detained, cooperating, much, thaler, decrypt,\n",
      "Nearest to called: and, UNK, vert, michelob, ursus, considered, supercar, antwerp, pulau, callithrix,\n",
      "Nearest to no: any, ever, another, upanija, palomino, empty, semiconductors, appealing, agp, hundreds,\n",
      "Nearest to some: many, these, several, abet, all, the, any, other, callithrix, manure,\n",
      "Nearest to would: will, can, could, may, might, should, must, to, did, does,\n",
      "Nearest to world: insights, scorn, stumbling, umlaut, pulling, customizable, transcends, peacocks, philippines, scripting,\n",
      "Nearest to there: they, it, he, often, still, now, usually, which, mico, these,\n",
      "Nearest to when: if, while, before, where, after, however, although, during, but, and,\n",
      "Nearest to time: peacocks, amalthea, microcebus, battalions, ursus, kapoor, year, aorta, delusions, tapioca,\n",
      "Nearest to at: in, during, michelob, on, thaler, callithrix, amalthea, abet, sheds, polyn,\n",
      "Nearest to not: generally, often, they, it, to, you, bpp, still, also, usually,\n",
      "Nearest to one: three, ursus, six, two, agouti, five, four, seven, iit, callithrix,\n",
      "\n",
      "Average loss at step  125000 :  4.55231066194\n",
      "Average loss at step  130000 :  4.52120715604\n",
      "Nearest to american: british, french, english, agave, kennel, german, simplified, european, adjudged, thibetanus,\n",
      "Nearest to that: which, however, what, but, bpp, this, callithrix, kapoor, then, alphorn,\n",
      "Nearest to may: can, would, could, will, might, should, must, cannot, does, did,\n",
      "Nearest to by: was, be, upanija, with, thaler, through, during, from, without, eventually,\n",
      "Nearest to into: from, through, under, nationalisation, troll, with, sponsors, against, arctos, mico,\n",
      "Nearest to first: second, last, next, archie, current, agave, agouti, upanija, invaluable, globemaster,\n",
      "Nearest to while: when, although, but, and, however, pulau, after, michelob, crb, mitral,\n",
      "Nearest to such: well, these, many, known, some, including, having, mosiah, responsa, other,\n",
      "Nearest to than: or, and, but, microsite, milne, much, cooperating, detained, thaler, decrypt,\n",
      "Nearest to called: UNK, vert, ursus, michelob, supercar, considered, and, antwerp, pulau, callithrix,\n",
      "Nearest to no: any, upanija, another, semiconductors, ever, palomino, appealing, agp, empty, hundreds,\n",
      "Nearest to some: many, these, several, abet, all, any, other, the, callithrix, most,\n",
      "Nearest to would: will, could, can, may, might, should, must, did, does, to,\n",
      "Nearest to world: insights, scorn, stumbling, pulling, umlaut, customizable, maynard, transcends, philippines, peacocks,\n",
      "Nearest to there: they, it, he, still, usually, often, now, mico, which, callithrix,\n",
      "Nearest to when: if, while, before, after, where, during, although, however, but, within,\n",
      "Nearest to time: peacocks, year, battalions, microcebus, kapoor, amalthea, ursus, aorta, period, rotate,\n",
      "Nearest to at: in, during, michelob, on, polyn, amalthea, abet, thaler, emulsifiers, under,\n",
      "Nearest to not: generally, often, it, to, bpp, they, usually, still, you, always,\n",
      "Nearest to one: seven, six, two, four, eight, agouti, ursus, five, three, microcebus,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_steps = 130001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # We must initialize all variables before we use them.\n",
    "    session.run(init)\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    print('Initializing the model')\n",
    "    \n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_samples, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "        \n",
    "        # We perform one update step by evaluating the optimizer op using session.run()\n",
    "        _, loss_val, summary = session.run([optimizer, loss, merged_summary_op], feed_dict=feed_dict)\n",
    "        \n",
    "        summary_writer.add_summary(summary, step )\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 5000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 5000\n",
    "            \n",
    "                # The average loss is an estimate of the loss over the last 5000 batches.\n",
    "                print('Average loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "\n",
    "        # Evaluate similarity after every 10000 iterations.\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval() #\n",
    "            for i in range(sample_size):\n",
    "                sample_word = reverse_dictionary[sample_examples[i]]\n",
    "                top_k = 10  # Look for top-10 neighbours for words in sample set.\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % sample_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "            print()\n",
    "            \n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Graph, and Loss function in TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Open a terminal in current working directory, and follow these steps:<br> \n",
    "1. Run following command: **tensorboard --logdir=./log/** <br>\n",
    "2. Open http://127.0.0.0:6006/ into your web browser.<br>\n",
    "3. Visualize the ** Model Graph** under **GRAPHS** tab.<br> \n",
    "4. Visualize plot of **loss** under **SCALARS** tab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
